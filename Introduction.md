The efficiency of statistical software, particularly in terms of time and memory usage, is a critical factor in computational research and data analysis. As the scale and complexity of data continue to grow, the need for optimized algorithms and implementations becomes very important. Thus, it is important we Benchmarking the performance of our  software, especially statistical software, where time and memory efficiency becomes critical. When different implementations of an algorithm exist, benchmarking helps to determine which one performs best for a given task. For example, consider the task of writing a large csv file of dataset. Different algorithms—such as pandas::pd.DataFrame.to_csv(), dask::dask.dataframe.to_csv(), write.csv, data.table:fread, utils:read, and readr::write_csv —can accomplish this, but their time and memory consumption vary significantly depending on the data size. A typical benchmarking tool in R, like microbenchmark, can be used to compare these implementations by running each on a dataset of fixed size $N$ and reporting metrics such as execution time and memory usage.
However, such tools are often limited to comparisons at a single data size, $N$, making it difficult to understand how performance scales as $N$ increases. This is where asymptotic benchmarking comes into play. For instance, in addition to benchmarking writing a csv file at $N = 1000$, we can also run them at $N = 10,000$, $N = 100,000$, and so on. By doing this, we can observe not just how fast an algorithm performs at one size, but how its performance changes as the dataset grows. This type of analysis is crucial for estimating complexity classes, such as big-O notation, which tells us whether the time or memory usage grows linearly, logarithmically, or exponentially with $N$.

Various tools and frameworks have been developed to address the need for benchmarking, each with its own approach to assessing computational efficiency. These benchmarking tools typically focus on comparing the performance of different computations for a fixed data size N. While useful, this approach may not capture the full picture of an algorithm's efficiency across varying data scales, also these tools are either designed for comparative benchmarking, performance testing  or continuous performance testing, not all. 

\paragraph{Performance testing.}

Performance tests aim to assess a package repository for available version releases and benchmark its performance, including gathering information on memory usage and execution time, of this version release.
for example in memory usage, the test is to evaluates the memory used by a package during execution. For instance, using tools like \pkg{memory\_profiler} \citet{memory_profiler} in Python, one can track how much memory is consumed at various points in the code. Measuring execution time, involves the time taken for specific functions or processes takes to complete. For example, employing the \pkg{timeit} \citet{timeit} module in Python allows users to run a piece of code multiple times and calculate the average execution time.

\paragraph{Comparative benchmarking.}

In Comparative benchmarking, we compare and visualize the asymptotic performance (time and memory usage) of the different functions, By comparing the asymptotic performance of these functions in various or particular programming languages, with the aim of provide insights into their usage and to help data scientists make informed choices when it comes to data
manipulation and analysis. In R \pkg{profvis} \citet{profvis} is used can be used for memory measurement and microbenchmark \citet{microbenchmark} for time measurement.

The \pkg{atime} package was developed specifically to address the limitations of traditional benchmarking tools. It allows users to measure performance for a sequence of increasing data sizes, providing insights into both time and memory usage over a range of $N$. For example, by using \pkg{atime\_versions()}, developers can compare the performance of different git versions of their package as the input size grows, and estimate the complexity class of each. The function \pkg{atime()} also allows developers to compare time and memory usage for R code that varies based on input size. Additionally, \pkg{references\_best()} offers asymptotic complexity estimates, giving developers valuable insights into how their algorithms scale as the input size increases. This features make \pkg{atime} particularly useful for developers who want to optimize R packages for large-scale data analysis.
\code{atime} also integrates GitHub Actions which we call the continuous performance testing, enabling developers to run \pkg{atime\_pkg} during pull requests. This automated approach ensures that performance regressions are detected early in the development process, saving time and reducing the likelihood of bugs making their way into production.

In this paper, we present a detailed comparison of \pkg{atime} with existing benchmarking tools, demonstrating how it can be used to improve performance in projects like base R and data.table.


