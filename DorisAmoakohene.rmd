---
author:
- Doris Amoakohene, Toby Hocking, Anirban Chetia
bibliography:
- DorisAmoakohene.bib
title: atime for asymptotic timings
---

When developing R packages, understanding how your code performs is
important. This is why analyzing time complexity is beneficial, and
asymptotic timings help us understand how algorithms scale as data size
increases, providing performance insights. Previous work has produced
various packages and functions for performance benchmarking, yet we
often do not prioritize time and memory complexity, even though they are
important. We propose a new package `atime`, for asymptotic timing,
performance testing and comparative benchmarking. We are evaluating the
features of `atime` against previous functions and packages,
investigating the GitHub Action that uses it, and comparing its
performance with other R packages that utilize GitHub Actions.

# Introduction

The performance of an R package provides utilities for computing
measures to assess model quality, many of which are not directly
provided by R's base or stats packages as discussed by [@system.time].
In the field of computer science and algorithm development, performance
analysis plays a crucial role in determining the efficiency and
effectiveness of different algorithms as mentioned in [@knuth1997art].
Performance analysis helps us identify the most efficient algorithms for
a given task, optimize code versions, and make informed decisions about
algorithm selection in real-world applications. Several factors
contributing to an algorithm's performance, including its time and
memory complexity are discussed in the latest edition of introdution to
algorithms [@cormen2022introduction].\
Understanding the atime package requires knowledge of asymptotic
complexity, also known as \"Big O\" notation. Asymptotic complexity
describes how an algorithm's runtime or memory usage scales as the size
of the input data increases. Analyzing performance characteristics,
including time and space complexity, is critical when working with large
datasets. In [@Hocking2021], his analysis provides valuable insights
into trade-offs between convenience and computational efficiency,
enabling users to make informed decisions when selecting data reshaping
tools.

[@suh2017emp] discusses the importance of accurately measuring a program
execution time, a common performance evaluation technique in computer
science research. They highlight the distinction between elapsed time
(ET) and process time (PT), where ET represents the end-to-end time of a
program, while PT focuses solely on the execution time of the process of
interest, atime addresses this challenge by providing a powerful tool
for analyzing and comparing the asymptotic time and memory usage of
different R codes.

Several packages such as microbenchmark by [@microbenchmark], bench
developed by [@bench], and system.time by [@system.time] exist for
benchmarking asymptotic timings. This section discusses these packages
and compares them with atime. Unlike the functions from these packages
which compare timings for a single value, atime allows users to check
execution time over a sequence of data sizes (N) and visualize the
results.\
The atime package is designed to analyze algorithm asymptotic time
complexity. It provides a simple and efficient way to measure code
performance, enabling users to understand how it scales with increasing
input size. `atime` works by repeatedly executing a function or
algorithm with varying input sizes and measuring the time taken to
complete each run.\
This article provides an overview and description of the atime package
and its applications, explaining its use through examples with R
functions. We pursue two key objectives: comparative benchmarking and
performance testing with atime. Through comparative benchmarking, we
compare package performance, providing insights into their usage and
empowering data scientists to make informed decisions. Performance
testing enables us to detect and prevent significant performance
regressions, ensuring that slowness or excessive memory usage does not
compromise user experience.\

# Related work

Various functions and packages in R and other programming languages like
Python offer a range of tools for performance testing and comparative
benchmarking. These include:\

::: {#tab:my_label}
                      Project                       GitHub action        benchmarking technique                             
  ------------------- ----------------------------- -------------------- -------------------------------------------------- --
  bench               R                             \-                   Comparative benchmarking                           
  microbenchmark      R                             \-                   Comparative benchmarking                           
  system.time         R                             \-                   Comparative benchmarking                           
  rbenchmark          R                             \-                   Comparative benchmarking                           
  airspeed velocity   python                        custom web page      performance testing                                
  Conbench            arrow                         \-                   performance testing                                
  touchstone          R                             PR comment.          performance testing                                
  pytest-benchmark    Python projects with pytest   custom github page   performance testing                                
  atime(proposed )    R                             PR comment,.         performance testing and Comparative benchmarking   

  : Comparing atime and other packages
:::

[]{#tab:my_label label="tab:my_label"}

**Goal of the Table:** in Table 1, the goal is to compare different
benchmarking technologies across various software, functions, and
packages. This helps in understanding the strengths and weaknesses of
each function or package in comparison to `atime`, our proposed package.

**Expected Observations:** We expect to see that each benchmarking
technology has its own set of strengths that make it suitable for
specific tasks. For example, some functions or packages might be better
suited for only comparative benchmarking, while others might excel in
performance testing. However, `atime` is suited for both comparative
benchmarking and performance testing.

**Actual Observations:** In the table below, you can observe the
following comparisons:

**System Time:**\
Used for comparative benchmarking in R packages and GitHub Actions.

**Microbenchmark:**\
Used for comparative benchmarking in R packages.

**Airspeed Velocity:**\
Used for performance testing in Python with a custom web page.

**Conbench:**\
Used for performance testing in Arrow.

**Touchstone:**\
Used for performance testing in R with PR comments.

**`atime` (proposed):**\
Used for performance testing and comparative benchmarking in R with PR
comments.

**Conclusion:** From this table, we conclude that different benchmarking
technologies offer various capabilities for performance testing and
benchmarking. The proposed `atime` package stands out as it combines
both performance testing and comparative benchmarking, making it a
versatile choice for different benchmarking needs. This understanding
can help you choose the most appropriate technology for your specific
benchmarking tasks.

## Comparative Benchmark

**bench:**\
@bench proposed the `bench` package in R, which provides a flexible
benchmarking tool to measure time and memory usage of code, including
`bench_mark()` for basic benchmarking, `press()` for repeated
benchmarks, and `autoplot()` for visualizations. It offers functionality
which focuses solely on time and memory requirements for a single data
size.

**microbenchmark**\
@microbenchmark proposed the `microbenchmark`, this is an R package that
provides nanosecond-precision timing of multiple R expressions, with
controls such as randomization of execution order. This allows for
precise and robust measurement of execution time. In relation to our
proposed package `atime`, `microbenchmark` shares similarities with
`bench` in terms of performance measurement. However, unlike `bench`,
`microbenchmark` focuses solely on execution time and does not measure
memory usage. Our `atime` package builds upon this concept, providing
asymptotic timing measurements to analyze the time and memory usage of R
code.

**system.time**\
@system.time is a base R function which provides a quick way to measure
the execution time of code but has a relatively coarse resolution in the
millisecond range. In contrast, the proposed `atime` functionality aims
to provide a benchmarking tool, with the ability to measure both time
and memory usage at a higher, microsecond-level resolution. This higher
precision allows `atime` to detect smaller performance differences that
may be missed by the cruder `system.time()`. While `system.time()` is
suitable for basic performance evaluation, `atime` is designed to offer
a more detailed and accurate assessment of code performance.

## Performance Testing

**timeit**\
@timeit proposed the timeit module in Python which measures execution
time with lower resolution than atime. While timeit calculates average
execution time, atime provides asymptotic timing measurements for more
accurate analysis. atime is designed for R code, whereas timeit is
specific to Python. atime offers more detailed insights into performance
differences than timeit. By providing higher resolution measurements,
atime is a more advanced tool for performance analysis.

**profile.run**\
@profile developed the profile module in Python's standard library, this
is used as a profiling tool that provides detailed information about the
execution of one's code. The profile.run function in Python provides
detailed information about execution, but its focus is on profiling
rather than benchmarking. In contrast, atime is specifically designed
for benchmarking R code with high accuracy. While profile.run includes
time spent in each function, atime provides more detailed insights into
performance differences. atime is designed for R code, whereas
profile.run is specific to Python. By providing more accurate
measurements, atime is a more advanced tool for performance analysis.

**Airspeed Velocity**\
@airspeed_velocity created a Python library for measuring the
performance of code. This function measures performance of code with
lower resolution than atime. While Airspeed Velocity allows writing
benchmarks, atime provides asymptotic timing measurements for more
accurate analysis. atime is designed for R code, whereas Airspeed
Velocity is specific to Python. atime offers more detailed insights into
performance differences than Airspeed Velocity. By providing higher
resolution measurements, atime is a more advanced tool for performance
analysis.

**pytest-benchmarks**\
@pytest_benchmark developed the Pytest-benchmarks which integrates
Airspeed Velocity benchmarking into pytest, but its resolution may not
be sensitive enough to detect small differences. In contrast, atime
offers higher resolution and more precise asymptotic timing
measurements. While pytest-benchmarks tracks performance regressions,
atime provides more detailed insights into performance differences.
atime is designed for R code, whereas pytest-benchmarks is specific to
Python. By providing more accurate measurements, atime is a more
advanced tool for performance analysis.

**conbench**\
@conbench proposed conbench, it's focuses on comparing machine learning
models, whereas atime is designed for benchmarking R code. While
conbench provides standardized benchmarking, atime offers higher
resolution and more precise asymptotic timing measurements. atime
provides more detailed insights into performance differences than
conbench. By providing more accurate measurements, atime is a more
advanced tool for performance analysis. atime is specific to R code,
whereas conbench is designed for machine learning models.

**touchstone**\
@touchstone created an R package touchstone which is used as a
continuous benchmarking tool that focuses on reliable relative
measurement and uncertainty reporting, whereas atime provides asymptotic
timing measurements for more accurate analysis. While touchstone tracks
performance changes, atime offers more detailed insights into
performance differences. atime is designed for R code, whereas
touchstone is also an R package. By providing higher resolution
measurements, atime is a more advanced tool for performance analysis.
atime complements touchstone and offers a more advanced solution for
performance analysis.

When selecting an appropriate package or function for performance
testing, researchers should consider the specific requirements of their
benchmarking task. Factors to weigh include the desired level of
precision, the complexity of the code being benchmarked, and the need
for visualization or statistical analysis of the results. This is why we
recommend the atime package and its functions for benchmarking.

## Features of the Package

The package in R offers several key features that make it a valuable
tool for comparative asymptotic timing analysis.

**Asymptotic Time and Memory Measurement:**\
The primary function in the atime package is `atime()`, which allows you
to measure the time and memory usage of R expressions as the size of the
input data increases. This enables you to understand the asymptotic
complexity of your code and identify potential performance bottlenecks.

**Asymptotic Reference Lines:**\
The `references_best()` function in atime fits asymptotic reference
lines to the time and memory measurements, providing a concise way to
visualize and interpret the complexity of your code. These reference
lines can help you quickly identify the dominant factors driving
performance and make informed decisions about optimizing your code.

**Estimate max data size for a given time/memory limit:**\
The `predict()` function in atime allows you to estimate the data size
required to achieve a target time or memory usage, based on the fitted
asymptotic reference lines. This can be particularly useful for capacity
planning and understanding the scalability of your R-based applications.

**Version Comparison:**\
The `atime_versions()` function enables you to compare the performance
of different git commits, helping you understand the impact of code
changes on your application's overall efficiency and scalability.

**Performance Testing:**\
The `atime_pkg()` function provides a way to integrate asymptotic timing
and memory analysis into your workflows, ensuring that new changes to
your R packages do not introduce performance regressions.

# Proposed method: atime for measuring asymptotic time and memory

# Results / empirical comparisons

This section presents the empirical findings and comparative analyses of
the atime package, demonstrating its effectiveness in evaluating the
performance of various algorithms and package versions. Using the
atime() function, we captured the execution times and memory usage of
different code algorithms, providing a comprehensive understanding of
their performance. The subsequent plots and visualizations will
illustrate the comparative benchmarking and performance testing results,
highlighting the performance improvements over time.\
One of the core functions of the `atime` package is the `atime` and
`atime_versions` functions, which allow you to measure the time and
memory usage of R expressions and versions, respectively, as the size of
the input data increases.

Here's a general overview of how to use `atime` and `atime_versions` to
analyze the performance of different manipulation algorithms:\
Basic parameters for which you can specify when using a time:

-   *N* is a numeric vector of data sizes to vary.

-   *setup* is an expression to evaluate for every data size before
    timings.

-   *times* is the number of times each expression is timed (so we can
    take the median and ignore outliers).

-   *seconds.limit* is the maximum number of seconds. If an expression
    takes more time, then it will not be timed for larger *N* values.

-   there should also be at least one other named argument (an
    expression to time for every size *N*, the name is the label which
    will appear on plots).

-   *pkg_edit_fun*: \@Toby, this section

```{=html}
<!-- -->
```
-   **Visualizing Asymptotic Complexity:** The atime package also
    provides tools for visualizing the asymptotic complexity of your R
    code. The plot() function can be used to generate plots of the time
    and memory usage measurements. The time and memory usage
    measurements are visualized using the command `plot(atime.list)`,
    where `atime.list` is the output list containing the results.

-   **Fitting Asymptotic Reference Lines:** To better understand the
    asymptotic complexity of your code, you can use the
    references_best() function to fit asymptotic reference lines to the
    time and memory usage measurements:

```{=html}
<!-- -->
```
    Fit asymptotic reference lines
    >best.list <- atime::references_best(atime.list)
    >plot(best.list)

![showing the best fit asymptotic time complexity for each linear and
constant timings](best.list.R.png){#fig:label1 width="0.8\\linewidth"}

In the figure above, the plot shows the timings of each expression as a
function of data size N

**Goal of the Figure:** The goal of creating this figure is to
understand how computational time varies with dataset size and infer how
different algorithms scale with larger data amounts.

**Expected Observations:** As dataset size increases, computational time
is expected to increase minimally for constant complexity algorithms and
proportionally for linear complexity algorithms.

**Actual Observations:** The figure shows that constant complexity
algorithms maintain consistent computational times regardless of dataset
size, while linear complexity algorithms' times increase steadily with
larger datasets.

**Conclusion:** Algorithms with constant complexity are highly efficient
for large datasets due to consistent computational times, while linear
complexity algorithms become less practical as dataset size grows due to
increasing computation times.

## Comparative Benchmarking

Comparative benchmarking is the process of comparing the performance of
different algorithms or approaches for benchmarking. It allows us to
assess packages performance against different packages and algorithms by
measuring execution time and memory usage.\
atime offers comparative benchmarking technique to evaluate packages,
providing insights into performance efficiency. By conducting a
comparative benchmark these tests, we ensure package remains a
high-performance tool for data analysis, delivering reliable results for
end-users\

### Comparing R packages performing similar Task.

When working with data in R, we all want to know which tools are the
most efficient. That is why we're putting some of the most popular R
packages to the test. In this section, we would demonstrate how to use
atime for comparative benchmarking for three R functions that perform
the same task. We will demonstrate the advantages of using atime for
comparative benchmarking over other R packages. The major advantage is
it allows us to alternate the expressions over different N sizes.
Another advantage is it gives you a more appropriate and easy
interpretation of your results visually.\
For example, consider the following simple example of comparatively
benchmarking three functions from the R packages , and for reading a CSV
file. Our goal is to provide an idea as to which packages are the
fastest and most memory-efficient, so people can choose the best ones
for their data analysis needs.\

``` {.r language="R"}
> n.rows <- 100
> seconds.limit <- 5
> atime.read.vary.cols <- atime::atime(
+   N = as.integer(10^seq(2, 6, by = 0.5)),
+   setup = {
+     set.seed(1)
+     input.vec <- rnorm(n.rows * N)
+     input.mat <- matrix(input.vec, n.rows, N)
+     input.df <- data.frame(input.mat)
+     input.csv <- tempfile()
+     fwrite(input.df, input.csv)
+   },
+   seconds.limit = seconds.limit,
+   "data.table::fread" = {
+     data.table::fread(input.csv, showProgress = FALSE)
+   },
+   "readr::read_csv\n(lazy=TRUE)" = {
+     readr::read_csv(input.csv, progress = FALSE, show_col_types = FALSE, lazy = TRUE)
+   },
+   "utils::read.csv" = utils::read.csv(input.csv)
+ )
```

The code above uses the `atime` package to measure and compare the
performance (time and memory usage) of different R functions for reading
CSV files as the size of the input data increases. Here's a step-by-step
breakdown:

-   **Parameters:**

    -   `n.rows` is set to 100, representing the number of rows in the
        input data.

    -   `seconds.limit` is set to 5, the maximum time allowed for each
        operation.

-   **Defining the `atime.read.vary.cols` Object:**

    -   The `atime::atime` function is used to benchmark different
        reading functions.

    -   `N` is a sequence of increasing sizes of data ($10^2$ to $10^6$,
        incremented by 0.5 powers of 10).

-   **Setup:**

    -   A random seed is set for reproducibility.

    -   A vector of random numbers is generated and reshaped into a
        matrix with `n.rows` rows and `N` columns.

    -   This matrix is converted into a data frame.

    -   The data frame is written to a temporary CSV file.

-   **Benchmarking:**

    -   Three different functions are benchmarked for reading the CSV
        file:

        -   `data.table::fread`: Reads the CSV using the `data.table`
            package.

        -   `readr::read_csv`: Reads the CSV using the `readr` package
            with lazy evaluation.

        -   `utils::read.csv`: Reads the CSV using the base R function.

```{=html}
<!-- -->
```
    List of 3
     $ unit.col.vec : Named chr [1:2] "kilobytes" "median"
      ..- attr(*, "names")= chr [1:2] "" "seconds"
     $ seconds.limit: num 5
     $ measurements :Classes ‘data.table’ and 'data.frame': 19 obs. of  17 variables.

-   **unit.col.vec:** Named character vector of length 2, with values
    \"kilobytes\" and \"median\".

    -   The attributes contain names, which are \"\" and \"seconds\".

-   **seconds.limit:** Numeric value of 5, indicating the maximum
    allowed time for each operation.

-   **measurements:** A `data.table` and `data.frame` with 19
    observations and 17 variables.

The output is a list of three elements, including `unit.col.vec`,
`seconds.limit`, and `measurements`, of which each measurement is of
class `data.table` and `data.frame`. The `unit.col.vec` is a named
character vector with units (\"kilobytes\" and \"median\"), and
`seconds.limit` is a numeric value set to 5. The `measurements` contain
19 observations and 17 variables. These data can be used to create the
desired plot with `ggplot2`.

![Comparing different functions for reading a csv
file](gg.read.3.png){#fig:label2 width="0.7\\linewidth"}

**Goal of the Figure:** The goal of this figure is to compare the
computational time of different data reading functions in R
(`utils::read_csv`, `readr::read_csv`, and `data.table::fread`) as the
number of columns increases.

**Expected Observations:** As the number of columns increases,
computational time is expected to increase for each function, but at
potentially different rates, with `data.table::fread` expected to be the
most efficient.

**Actual Observations:** The figure shows that `data.table::fread` has
the lowest computational time, followed by `readr::read_csv` and then
`utils::read_csv`, with increasing divergence as the number of columns
increases.

**Conclusion:** `data.table::fread` is consistently the most efficient
function for reading large datasets with many columns, maintaining lower
computational times compared to the other functions.

## Performance Testing

The atime package includes the atime_pkg() function, which enables you
to integrate asymptotic timing and memory analysis into your continuous
integration and deployment workflows. This can help ensure that new
changes to your R packages do not introduce performance regressions.
When performing these performance tests, we use the
atime::atime_versions. Performance testing can be done on any R package
written with Rcpp.\
we will discuss how to use atime::atime_versions to create performance
testing for the data.table package in R. data.table is a powerful
extension of R's data.frame, designed to handle large datasets with
exceptional efficiency. Its concise and expressive syntax enables users
to perform complex data manipulations with ease, making it an ideal tool
for data analysis. The development team behind data.table is dedicated
to continuously optimizing its performance, ensuring swift execution of
tasks like filtering, grouping, aggregating, and joining data. To
guarantee high-performance standards, data.table employs a robust
performance testing framework. This framework measures the actual
execution time of specific operations, enabling accurate comparisons
between different package versions. Our atime performance tests aim to
assess package repositories by benchmarking performance and gathering
information on memory and time usage. By conducting these tests, we gain
valuable insights into a package's performance efficiency, ensuring the
delivery of a reliable package to users.

Approach

-   *To begin* conduct the atime test for the different code branches
    (before regression, regression, fix regression) to identify
    potential performance issues.

    Note: Set up the necessary environment and dependencies, ensuring
    that the data.table and atime packages are installed and loaded.

-   *Generate* a plot to showcase the fixes made in the data.table
    package using the atime package.

-   *utilize* the atime::atime_versions function to track the fixes
    across different versions.

-   *pass* the following named arguments to atime::atime_versions: N,
    setup, expr, and the different code branches. More documentation of
    the atime package can be found
    \[here\](https://github.com/tdhock/atime).

-   *use* the plot function to visually present the execution times of
    the expression evaluated across different versions of the data.table
    package

**What are the Performance Tests?**

Our `atime` performance tests aim to assess the `data.table` repository
by benchmarking its performance and gathering information on memory and
time usage. By conducting these tests, we can gain insights into the
package's performance efficiency.

When using `atime_versions`, there are six main arguments:

1.  **pkg.path:** This argument specifies the location on your system
    where you have stored a git clone of the `data.table` package.

2.  **expr:** This section contains the expression that represents the
    operation being benchmarked. It uses the `data.table::[.data.table]`
    syntax to perform the operation on the dataset. In the given syntax
    `data.table::[.data.table]`, the first part `data.table::` installs
    and loads different versions of the `data.table` package based on
    the specified commit IDs. Hence, `data.table::` will be translated
    to `data.table.SHA1::` for some version hash SHA1. Following that,
    the expression specified within `[.data.table]` is executed on each
    installed version. This process is repeated for all the specified
    commit IDs in the code.

3.  **\... :** This section specifies the different versions of the
    `data.table` packages that will be tested. It includes three
    versions: "Before," "Regression," and "Fixed." Each version is
    associated with a specific commit ID.

We run the full performance regression:

1.  Before the performance regression is made (Before)

2.  When the performance regression is first submitted (Regression/Slow)

3.  Pull Request (PR) which fixes the performance regression
    (Fixed/Fast)

Performance Test case with two code branches: fast and slow:\

``` {.r language="R"}
> atime.list <- atime::atime_versions(
+   pkg.path = "~/data.table",
+   pkg.edit.fun = pkg.edit.fun,
+   N = 10^seq(1, 7, by = 0.25),
+   setup = { 
+     DT <- replicate(N, 1, simplify = FALSE)
+   },
+   expr = data.table:::setDT(DT),
+   "Slow" = "c4a2085e35689a108d67dacb2f8261e4964d7e12",
+   "Fast" = "1872f473b20fdcddc5c1b35d79fe9229cd9a1d15"
+ )
```

The code above benchmarks the performance of different versions of the
data.table package by measuring the computational time for a specific
setDT operation on a data.table with varying numbers of rows. It helps
in understanding the impact of the regression and the effectiveness of
the fix.\

![Performance Test case with two code branches: fast and
slow](atime.list.5427.png){#fig:label3 width="0.6\\linewidth"}

**Goal of the Figure:** The goal of this figure is to compare the
computational time of two versions of a package, one with a regression
causing slowness ("Slow") and a newer version ("Fast") released to
mitigate the regression and improve performance. This comparison helps
understand the impact of the regression and the effectiveness of the new
version.

**Expected Observations:** As expected, the "Slow" version exhibits
higher computational times as the number of rows increases, while the
"Fast" version maintains lower computational times, demonstrating
improved performance and efficiency.

**Actual Observations:** The figure shows that for smaller datasets,
both versions have similar computational times, but as the number of
rows grows, the "Slow" version becomes significantly slower, whereas the
"Fast" version maintains a lower computational time.

**Conclusion:** The comparison clearly shows that the new "Fast" version
successfully mitigates the regression and enhances performance, making
it a better choice for data processing tasks involving large datasets.
The output will capture the computational time for each version of the
data.table package (Fast and Slow) across different dataset sizes.

Performance Test case with three code branches: Regression, Fixed and
Before:\

``` {.r language="R"}
> atime.list <- atime::atime_versions(
+   pkg.path = "~/data.table",
+   pkg.edit.fun = pkg.edit.fun,
+   N = 10^seq(1, 7, by = 0.25),
+   setup = { 
+     set.seed(108)
+     d <- data.table(
+       id3 = sample(c(seq.int(N * 0.9), sample(N * 0.9, N * 0.1, TRUE))),
+       v1 = sample(5L, N, TRUE),
+       v2 = sample(5L, N, TRUE))
+   },
+   expr = data.table:::`[.data.table`(d, , (max(v1) - min(v2)), by = id3),
+   "Before" = "793f8545c363d222de18ac892bc7abb80154e724",
+   "Regression" = "c152ced0e5799acee1589910c69c1a2c6586b95d",
+   "Fixed" = "f750448a2efcd258b3aba57136ee6a95ce56b302"
+ )
```

This code above is almost as the first simple, except it has three code
branches Before, Regression and Fixed. it benchmarks the performance of
different versions of the data.table package by measuring the
computational time for a specific operation on a data.table with varying
numbers of rows. The output as mentioned previously will capture the
computational time for each version of the data.table package (Before,
Regression, and Fixed) across different dataset sizes. It will compare
the performance of these versions, highlighting the impact of the
regression and the effectiveness of the fix.\

        data.table.793f8545c363d222de18ac892bc7abb80154e724::`[.data.table`(d, , (max(v1) - min(v2)), by = id3)  

In this code, the expression `[.data.table]` is executed on the `d`
dataset using the specified commit IDs
(793f8545c363d222de18ac892bc7abb80154e724) of the `data.table` package.
The expression calculates the difference between the maximum value of
`v1` and the minimum value of `v2`, and is grouped by `id3`. This
process is repeated for all commit IDs in the code to compare the
performance of different versions of the `data.table` package.\

![Performance Test case with three code branches: Regression, Fixed and
Before](atime.list.4200.png){#fig:label4 width="0.7\\linewidth"}

**Goal of the figure:** This figure compares the computational time of
three package versions: `Before`, `Regression`, and `Fixed` (after a
regression was improved or fixed), to understand the impact of the
regression and the effectiveness of the fix.

**Expected observations:** Computational time is expected to increase
during the `Regression` phase and decrease after the fix, either
returning to or improving upon the pre-regression time.

**Actual observations:** The figure shows three regions: a stable
`Before` region, a slower `Regression` region, and a faster `Fixed`
region, indicating the regression's impact and the fix's effectiveness.

**Conclusion:** The regression increased computational time, but the fix
successfully mitigated it, restoring or improving performance, making
the package more efficient for large datasets.

# Performance Testing with GitHub Actions CI

# Section by Anirban Chetia

A GitHub Action was created by Anirban to facilitate performance testing
of the incoming changes that are introduced via Pull Requests (PRs) to
the GitHub repositories of R packages. The primary motivation behind
this was to help ensure that , a popular R package, maintains its code
efficiency or high-performance standards (core values of the project) as
PRs keep coming and getting integrated frequently into the codebase,
meaning they need to be monitored for performance regressions, and an
automatic way to do that would be ideal. The key features of this GitHub
Action include:

-   **Predefined flexible tests** The action runs test cases (utilizing
    the package) from the setup defined in (can be customized) on
    different versions of or the R package being tested. These tests are
    either based on documented historical regressions or performance
    improvements.

-   **Automated commenting** Using , the action publishes results in a
    GitHub-bot authored comment on the pull request thread. The comment
    gets updated time and again on new pushes to avoid cluttering
    (ensuring only one comment exists per PR, updated with the latest
    information concisely).

-   **Diagnostic visualization** A plot is uploaded within the comment
    which comprises of subplots for each test case, showing the time and
    memory trends across different versions.

-   **Timing information** The time taken (in seconds) for setup and
    test execution is supplied within the comment as well.

-   **Links** A download link for the artifact containing all the
    -generated results is provisioned, apart from the GitHub link to the
    commit SHA that generated the latest plot results.

-   **Versioning** The action computes the tests on different versions
    that can be visually compared on the resultant plot. These include
    various labels, as described in the table below:

      Label name   R package version description
      ------------ ------------------------------------------------------------------------------------------------------------------------------------
      base         PR target
      HEAD         PR source
      Merge-base   The common ancestor between base and HEAD
      CRAN         Latest version on the CRAN platform
      Before       Pre-regression commit
      Regression   The commit (or if the source is unknown or distributed, the range of commits) which is responsible for the performance degradation
      Fixed        Commit where the performance has been restored or improved beyond the point of regression

      : Version labels

The action is not constrained to be OS-specific and there is only one
single job or set of steps that execute on the same runner. There are
ample examples of this in action within the official GitHub repository's
'Pull requests' section (actively running as new PRs involving code
changes emerge). The figure below is taken from one of those recent (at
the time of writing) runs.

![Test cases running for every code changing PR via the GitHub Action
that Anirban created](GHA2.png){#fig:label5 width="1.0\\linewidth"}

# Discussion and conclusions

In this paper, we described the `atime` package and its new functions
for comparative benchmarking, performance testing, and continuous
performance testing through GitHub Actions. We demonstrated how the
syntax of `atime` makes it easy to define and use its functions for
various types of performance evaluations.

Through several examples, we showcased the versatility of `atime` in
performing different comparisons and performance tests. Our detailed
comparison with other benchmarking packages and functions highlighted
the advantages of `atime`, including its ability to specify a range of
data sizes and facilitate result visualization.

We also demonstrated `atime`'s capacity to handle both constant and
linear asymptotic time complexity, making it a valuable tool for
performance analysis. Furthermore, we discussed the design choice to
keep separate functions for comparative benchmarking and performance
testing, allowing for more specific and informative documentation,
examples, and error messages.

`atime` features two distinct functions for benchmarking:

-   `atime::atime` for comparative benchmarking

-   `atime::atime_versions` for performance testing and continuous
    performance testing.

While it is possible to merge these functions into a single function, we
intentionally maintain their separation to provide detailed
documentation, relevant examples, and informative error messages,
ultimately enhancing the user experience.

Overall, our work introduces `atime` as a comprehensive and
user-friendly package for performance testing and comparative
benchmarking, offering a range of functions and features that make it an
essential tool for developers and researchers. By providing a detailed
understanding of `atime`'s capabilities and advantages, we hope to
encourage its adoption and contribute to the development of more
efficient and scalable software solutions.
